---
title: 数据重删论文1
date: 2017-03-27 11:00:41
tags: [ 数据重删,论文 ]
categories: 数据重删
---

# [Dmdedup: Device Mapper Target for Data Deduplication](http://www.fsl.cs.sunysb.edu/docs/ols-dmdedup/dmdedup-ols14.pdf)

## 概述

dmdedup这个软件是一个适用于常规应用者和研究人员的,多功能实用性的主存数据重删平台.在块级别进行操作,对应用和系统都是有用的.因为大多数据重删研究,设计和完善了一个后端接口,方便研究人员,构建和评估元数据管理的方法.
通过下面三个方式,完善了后端:
1. RAM_TAB:内存中的哈希表
2. DISK_TAB:磁盘哈希表
3. 基于B树的DISK_TAB:持续的写时拷贝的B树


主存重删与数据集备份相比带来的挑战有:
1. 访问局部性不太明显；
2. 延迟的限制更严格；
3. 更少的副本是可用的（约2×与备份10×）；
4. 和重复数据删除引擎必须与其他进程竞争CPU和RAM。

为了方便在主存储器中的重复数据删除技术的研究，我们开发了，这里提出了一个灵活的、完全可操作的主存储重复数据删除系统，Dmdedup，在Linux内核中实现。除了其吸引人的特性为普通用户，它可以作为实验的重复数据删除算法和研究主存储数据和工作负载的基础平台。

它和RAID , LVM 在同一层对linux内核块设备进行操作.

## 设计

### 分类

#### 数据重删层面
1. 应用层
2. 文件系统层
	实现方式:
		1. 修改现有的文件系统,比如EXT3或者WAFL.
		2. 在内核或者利用用户空间文件系统创建一个可折叠的文件系统.
		3. 从零开始实现一个重复数据删除的文件系统,比如,EMC 数据域的文件系统.
3. block level 
	缺点:
		1. 它必须保持一个额外的映射（超出文件系统的)逻辑和物理块之间的映射；
		2. 文件系统和应用程序有用的上下文丢失.
		3. variable length chunking is more difficult at the block layer. 
		
		
#### 时效性

### DM:设备映射器
Device Mapper 是 Linux2.6 内核中支持逻辑卷管理的通用设备映射机制，它为实现用于存储资源管理的块设备驱动提供了一个高度模块化的内核架构.

### Dmdedup的组成
![img](http://function.dearamaze.com/Dmdedup_struct.png)
1. 数据重删的逻辑,块存储,计算hash值,协调其他部件.
2. 一个hash表,用来索引hash值与块的位置.
3. 逻辑块号到物理块号的映射.
4. 空间管理器,在物理设备上索引空间,定位新的块,维护计数,回收没有引用的块.
5. 块存储.

### 写需求
### 分块
### 索引
Dmdedup软件里的hash值生成方法,它支持30多种哈希函数(内核的加密库),作者实验用的是128位的MD5 hash值.
hash值不能太短,容易发生碰撞,不能太长,增加了元数据的大小.
磁盘发生错误的概率是10^(-18)---10^(-15),128位的hash串,发生碰撞的概率低于磁盘发生错误的概率.

### 哈希索引和逻辑块号映射查询

1. 利用hash索引物理块号*PBN(new)*
2. 利用逻辑块号映射物理块号*PBN(old)*

### 元数据更新
![img](http://function.dearamaze.com/write_request_handling.png)

### 垃圾回收

垃圾回收,不是实时的,是一个离线的定时的回收.

## 读请求控制


## 后端元数据
### API
强制:
1. 初始化
2. 销毁
3. 块的定位,查找,插入,删除,关联数的控制

可选:
4. 垃圾回收
5. 元数据同步写入

### API类型
1. 线性(linear):Dmdedup uses a linear store (from zero to the size of the Dmdedup device)
for LBN mapping
2. 稀疏型(sparse):and a sparse one for the hash index.

### INRAM 
1. 重删的元数据一直存在RAM上
2. 允许我们自定义给多少CPU用于重删.
3. 他能很快算出工作负载的重删率
4. 在电容和电池的支持下可用于生产环境

用静态定位的哈希表用于键值存储,用一个数组用于线性存储.

线性映射数组的大小基于目标实例的大小.

用于离散存储的哈希表的大小基于数据设备的大小(预测最多可能有的不同数据块的数目).

用数组记录关联数

用数组定位新的块


### DTB
和INRAM用了相同的数据结构,但是是将数据保存在硬盘上(持久存储体).
没有缓冲区的话,每一次操作都是一次IO,严重影响dmdedup的性能. 我们用了Linux的`dm-bufio`子系统.

### CBT
[Linux写时拷贝技术(copy-on-write)](http://www.cnblogs.com/biyeymyhjob/archive/2012/07/20/2601655.html)

[B树](http://blog.csdn.net/v_JULY_v/article/details/6530142)

### 设备大小

设备逻辑大小可以改变

## 实验
### 实验组设置
1. 原生的设备
2. INRAM
3. DTB
4. CBT

元数据cache大小设置:
`0.3%` `25%` `50%` `75%` `100%` `135%`

### Dmdedup
通过连续的和随机的读写来测试其性能.

#### Micro-workloads

三种工作负载:
1. Unique,独一无二的数据:通过linux的`/dev/urandom` 设备产生的.
2. All-duplicates,全是重复的:每个块4kb,重复146GB
3. linux-kernel : 40个linux内核
 
两种类型:

1. 大量的连续写(I/O SIZE: 640KB)
2. 少量的随机写(I/O SIZE: 4KB)

##### Unique
连续写:

重删率是1,不包括元数据.这种情况下重删系统的性能是最差的,因为重删的工作还得做.INRAM的性能和原生设备的一样(这是在CPU和RAM足够快去做数据重删,没有任何明显的性能影响的情况下),CPU的使用率在65%.

元数据的更新确实是个瓶颈,4M大小的cache下,DTB的性能仅有原生设备的25%,cache大小在75%-100%之间,性能得到了很大的提升,因为100%的cache不需要元数据的读了,135%的cache下,元数据的写也避免了,所以DTB达到了INRAM的性能.

随机写:
原生的设备实现了420IOPS,Dmdedup比原生的要好很多,670-11100IOPS,因为他使随机的写变得有序,连续的定位新的块是重删系统的一个一般的策略.


#### All-duplicates 

#### Linux Kernels 

### Trace Replay 

1. Web
2. Mail
3. Homes 









